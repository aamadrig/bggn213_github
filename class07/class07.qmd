---
title: "Class 7: Machine learning 2"
author: "Assael Madrigal (PID: A10179083)"
format: pdf
---

#Clustering
k-means clusterine is very prevalent.
k means that we need to tell a k - how many groups I want, later we can tell it what it should be after analysing the output but we have to start with something.

To get started let's make some data, lets see how rnorm works by plotting a histogram
```{r}
#rnorm generates as many random numbers as I ask drawn from a normal distribution

hist(rnorm(10000, mean=3))
#so the mean is where the middle of the histogram is going to be
```

here we are going to make 2 groupings one centered around 3 and the other at -30
```{r}
tmp <- c(rnorm(30, mean=3), rnorm(30,-3))
x <- cbind(x=tmp, y=rev(tmp))
x
plot(x)
```

The main function in R for K-means clustering is called `kmeans()`, kmeans(x, centers,...). The center is the number of clusters.  nstart is the number of iterations kmeans will go, so one way is to keep increasing it until the answer does not change or! even better you can plot the "scree plot" and look at the elbow.

But it is still a limitation, but the advantage is that kmeans is very fast.
```{r}
k <- kmeans(x,centers=2, nstart = 20)
k
```

> Q1. How many points do I need to cluster? (from k)

```{r}
k$size
#a vector with the sizes of each cluster
```

> Q2. The clustering result ie membership vector?

```{r}
k$cluster
```

> Q3. Cluster centers?

```{r}
k$centers
```

> Q4. Make a plot of our data colored by clustering results with optionally the clusters centers shown

```{r}
#col is color but if we give it an number it has a color assigned to it, so for this case we can use it to color by the cluster if got assigned to
plot(x, col=k$cluster, pch=16)
points(k$centers, col="blue", pch=15, cex=2)
```

> Q.5 Run kmeans again but cluster into 3 groups and plot the results

```{r}
k3 <- kmeans(x,centers=3, nstart = 20)
plot(x, col=k3$cluster, pch=16)
points(k3$centers, col="blue", pch=15, cex=2)
```

The main problem with kmeans is that it will fit the data into the structure, so if we give it 3 clusters it will split it into 3 clusters. So on, so we have to be careful.

kmeans:
- breaks observations into k-predefined number of clusters
- you define the number of clusters!
- help by plotting with scree plots

# Heirarchical clustering
Heirarchical clustering has the advantage that it can potentially reveal structure in the data rather than imposing one as k-means will.

The main function in "base R" is  `hclust()`, is follows `hclust(d, method = "complete", members = NULL)` where 'd' was produced by `dist()` or any measure of dissimilarity.

It requires a distance matrix as input, not the raw data itself
```{r}
d<-dist(x)
hc <- hclust(d)
hc
```
```{r}
plot(hc)
```
The crossbar height is how far apart the datapoints are. So in this case the largest difference is in the first 2 clusters and then the smaller clusters are very little apart from there. Y axis, height, is the distance of the two branches below it.

There are two forms of hclust: bottom up vs top down

The function to get our clusters/groups from a hclust object is called `cutree()`
```{r}
cutree(hc, k=2)
#or
cutree(hc, h=8)
grps <- cutree(hc, k=2)
```
> Q. plot our hclust results in terms of our data colored by cluster membership

```{r}
plot(x, col=grps)
plot(x, col=grps, pch=15) #pch is just the shape of the points
```

# Principal Component Analysis (PCA)

# UK food class lab
first import the data
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

# Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?
```{r}
## Complete the following code to find out how many rows and columns are in x?
str(x)
dim(x)
head(x)
```
17 rows (observations) and 5 columns (variables)
it is using the names of the food as a column so we are going to make it a name 
instead 
```{r}
rownames(x)
```
I can change them like this: but this is overwriting x every time, because it is removing a column every time.
```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```
a better way to do this is: when we read the data we can tell it that the row names is in column 1 `row.names=1`
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names = 1)
head(x)
```
# Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I prefer assigning the row names at the same time as reading the data in. The problem may be if there are no rownames, but I would get the data first then visualize with with `head()` and then decide which column to pick for rownames

# Q3: Changing what optional argument in the above barplot() function results in the following plot? ()
I just set beside to False
```{r}
barplot(as.matrix(x), beside=F,col=rainbow(nrow(x)))
```

Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

So the diagonal means they consume the same amount for that particular food. Deviation from the diagonal is what is different between them. Each graph is comparing two countries to each other, but it is mirrored. The main thing is that the difference is what we are looking for.
```{r}
pairs(x, col=rainbow(10), pch=16)
```
#Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

Based on the graphs alone it is the food represented by the blue dot, the orange dot. but it is really hard to tell.




To help me makes sense of this data...
The main function for PCA in base R is called `prcomp()`

It wants the transpose (with the `t()`) of our food data for analysis
```{r}
pca <- prcomp(t(x))

summary(pca)
```
The proportion of variance is what i am looking at: 0.67 for PC1 it means that PC1 is responsible for 67% of the variance. The cumulative variance is the sum of them so for PC1 and PC2 it is 96.5%

One of the main results that folks look for is called the "score plot" a.k.a. PC plot, PC1 vs PC2 plot...

We are loooking at the distance from the point to the 0,0 
```{r}
plot(pca$x[,1], pca$x[,2])
abline(h=0, v=0, col="gray", lty=2) #adding a line just to see where0,0 is
```

# Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.
```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

#Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.
```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col=c("orange","red","blue","green"))
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
## or the second row here...
z <- summary(pca)
z$importance
#This information can be summarized in a plot of the variances (eigenvalues) with respect to the principal component number (eigenvector number), which is given below.
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```


To dig deeper we can see how much each variable affects the original PCA1 from the `$rotation` from `prcomp()` and summarized in `biplot()`
```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

The two biggest contributors are fresh fruit and soft drinks because they are the biggest bars.

# Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

Fresh potatoes makes the countries go left and soft drinks right so fresh potatoes differ the most between N. Ireland and Soft drinkg makes it the most different.
```{r}
#PC2
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```
PC2 is predominantly  still soft drinks but also alcoholic drinks.

```{r}
library(ggplot2)

df <- as.data.frame(pca$x)
df_lab <- tibble::rownames_to_column(df, "Country")

# Our first basic plot
ggplot(df_lab) + 
  aes(PC1, PC2, col=Country) + 
  geom_point()
```

```{r}
ggplot(df_lab) + 
  aes(PC1, PC2, col=Country, label=Country) + 
  geom_hline(yintercept = 0, col="gray") +
  geom_vline(xintercept = 0, col="gray") +
  geom_point(show.legend = FALSE) +
  geom_label(hjust=1, nudge_x = -10, show.legend = FALSE) +
  expand_limits(x = c(-300,500)) +
  xlab("PC1 (67.4%)") +
  ylab("PC2 (28%)") +
  theme_bw()
```
we can also make a nice ggplot for our pca
```{r}
ld <- as.data.frame(pca$rotation)
ld_lab <- tibble::rownames_to_column(ld, "Food")

ggplot(ld_lab) +
  aes(PC1, Food) +
  geom_col() 
```
and we can also make it nicer to see by color scaling and ordering them by highest to smallest
```{r}
ggplot(ld_lab) +
  aes(PC1, reorder(Food, PC1), bg=PC1) +
  geom_col() + 
  xlab("PC1 Loadings/Contributions") +
  ylab("Food Group") +
  scale_fill_gradient2(low="purple", mid="gray", high="darkgreen", guide=NULL) +
  theme_bw()
```


another way to visualize this is by using a biplot
```{r}
## The inbuilt biplot() can be useful for small datasets 
biplot(pca)
```

# PCA for RNA-seq
loading the data
```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

Q10: How many genes and samples are in this data set?
```{r}
class(rna.data)
str(rna.data)
dim(rna.data)
```
There are 100 genes and 10 samples (columns)
